# -*- coding: utf-8 -*-
# coding=utf-8
# å°†.pthæ–‡ä»¶è½¬æ¢ä¸º.ckptæ–‡ä»¶
import torch
import mindspore
import mindspore as ms
print(ms.__version__)

from mindspore.train.serialization import save_checkpoint
from mindspore.dataset.text import BertTokenizer
import pickle

file_lst = ["reftext_subtest_home.pth", "reftext_subtest_oov.pth", "reftext_subtest_other.pth",
            "reftext_subtest_semantic.pth", "reftext_subtest_shelf.pth", "reftext_subtest_sport.pth",
            "reftext_subtest_street.pth", "reftext_test.pth", "reftext_train.pth", "reftext_val.pth"]
output_lst = ["reftext_subtest_home.pkl", "reftext_subtest_oov.pkl", "reftext_subtest_other.pkl",
              "reftext_subtest_semantic.pkl", "reftext_subtest_shelf.pkl", "reftext_subtest_sport.pkl",
              "reftext_subtest_street.pkl", "reftext_test.pkl", "reftext_train.pkl", "reftext_val.pkl"]
def coverct_fun():
    for j in range(len(file_lst)):
        torch_model = torch.load(file_lst[j])
        print(type(torch_model))
        print(len(torch_model))
        for i in range(5):
            print(torch_model[i])

        # æ‰“å¼€ä¸€ä¸ªPickleæ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        with open(output_lst[j], "wb") as f:
            # å°†åˆ—è¡¨ä¿å­˜ä¸ºPickleæ–‡ä»¶
            pickle.dump(torch_model, f)

def test_berttok():

    tokenizer = BertTokenizer(vocab_file="vocab.txt")
    tokens = tokenizer.encode("I have a new GPU!")
    print(tokens)

# import mindspore.dataset as ds
# import mindspore.dataset.text as text
# from mindspore.dataset.text import NormalizeForm
#
# text_file_list = ["./vocab.txt"]
# text_file_dataset = ds.TextFileDataset(dataset_files=text_file_list)
#
# # 1) If with_offsets=False, default output one column {["text", dtype=str]}
# vocab_list = ["åºŠ", "å‰", "æ˜", "æœˆ", "å…‰", "ç–‘", "æ˜¯", "åœ°", "ä¸Š", "éœœ", "ä¸¾", "å¤´", "æœ›", "ä½",
#               "æ€", "æ•…", "ä¹¡","ç¹", "é«”", "å­—", "å˜¿", "å“ˆ", "å¤§", "ç¬‘", "å˜»", "i", "am", "mak",
#               "make", "small", "mistake", "##s", "during", "work", "##ing", "hour", "ğŸ˜€", "ğŸ˜ƒ",
#               "ğŸ˜„", "ğŸ˜", "+", "/", "-", "=", "12", "28", "40", "16", " ", "I", "[CLS]", "[SEP]",
#               "[UNK]", "[PAD]", "[MASK]", "[unused1]", "[unused10]"]
# vocab = text.Vocab.from_list(vocab_list)
# tokenizer_op = text.BertTokenizer(vocab=vocab, suffix_indicator='##', max_bytes_per_token=100,
#                                   unknown_token='[UNK]', lower_case=False, keep_whitespace=False,
#                                   normalization_form=NormalizeForm.NONE, preserve_unused_token=True,
#                                   with_offsets=False)
# text_file_dataset = text_file_dataset.map(operations=tokenizer_op)
# print(text_file_dataset)
#
# for data in text_file_dataset.create_dict_iterator():
#     print(type(data))
#     print(data["text"])



# å¯¼å…¥transformersåº“å’ŒBertTokenizerç±»
# from torchvision.transforms import Compose, ToTensor, Normalize
# input_transform = Compose([
#         ToTensor(),
#         Normalize(
#             mean=[0.485, 0.456, 0.406],
#             std=[0.229, 0.224, 0.225])
#     ])
# print(input_transform)

from mindspore import Tensor
from mindspore.ops import operations as P

x = Tensor([1, 2, 3, 4, 5])
y = P.clip_by_value(x, 2, 4)

print(y)




